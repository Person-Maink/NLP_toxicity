


import warnings
!pip install bitsandbytes
!pip install captum
import bitsandbytes as bnb
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

from captum.attr import (
    FeatureAblation,
    ShapleyValues,
    LayerIntegratedGradients,
    LLMAttribution,
    LLMGradientAttribution,
    TextTokenInput,
    TextTemplateInput,
    ProductBaselines,
)

# Ignore warnings due to transformers library
warnings.filterwarnings("ignore", ".*past_key_values.*")
warnings.filterwarnings("ignore", ".*Skipping this token.*")





import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig  # For bnb_config

# Check for device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Model name for Bloom 7B
model_name = "bigscience/bloom-7b1"

# Create the bnb_config
def create_bnb_config():
    return BitsAndBytesConfig(
        load_in_8bit=True,            # Enable 8-bit quantization
        llm_int8_threshold=6.0,      # Custom threshold for int8
        llm_int8_skip_modules=None,  # Set modules to skip for int8
        min_memory_reserved=1e9,     # Reserve memory for large models
        max_memory_reserved=1e10     # Max memory reserved
    )

# Load the model and tokenizer without token authentication
def load_model(model_name, bnb_config):
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",  # Automatically map layers to available GPUs
        quantization_config=bnb_config,  # Apply quantization config
        use_auth_token=False  # Explicitly disable token requirement
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=False)  # No token
    return model, tokenizer

# Instantiate bnb_config and load model/tokenizer
bnb_config = create_bnb_config()
model, tokenizer = load_model(model_name, bnb_config)

# Print success message
print("Model and tokenizer loaded successfully without a Hugging Face token!")






eval_prompt = "Dave lives in Palm Coast, FL and is a lawyer. His personal interests include"

model_input = tokenizer(eval_prompt, return_tensors="pt").to("cuda")
model.eval()
with torch.no_grad():
    output_ids = model.generate(model_input["input_ids"], max_new_tokens=15)[0]
    response = tokenizer.decode(output_ids, skip_special_tokens=True)
    print(response)





fa = FeatureAblation(model)

llm_attr = LLMAttribution(fa, tokenizer)





skip_tokens = [1]  # skip the special token for the start of the text <s>
inp = TextTokenInput(
    eval_prompt,
    tokenizer,
    skip_tokens=skip_tokens,
)

target = "playing guitar, hiking, and spending time with his family."

attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)





print("attr to the output sequence:", attr_res.seq_attr.shape)  # shape(n_input_token)
print("attr to the output tokens:", attr_res.token_attr.shape)  # shape(n_output_token, n_input_token)





attr_res.plot_token_attr(show=True)


import json
import pandas as pd

# New file paths based on the uploaded JSON files
file_paths = [
    '/content/bloom7b_eval_k50_p1_t1.0_n25_l50 (1).json',
    '/content/gpt2_eval_k50_p1_t1.0_n25_l150.json',
    '/content/mistral_eval_k50_p1_t1.0_n25_l150.json',
    '/content/llama_eval_k50_p1_t1.0_n25_l50.json'
]

# Function to extract the most toxic response for each prompt from a JSON file
def get_top_toxic_responses(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)

    rows = []

    for prompt in data.get('generated_responses', []):
        prompt_text = prompt.get('prompt', {}).get('text', '')
        continuations = prompt.get('continuations', [])

        # Find the response with the highest toxicity score for this prompt above the threshold
        toxic_responses = [
            {
                'Prompt': prompt_text,
                'Response': response.get('text', ''),
                'Toxicity Score': response.get('toxicity_score', 0),
                'File Path': file_path
            }
            for response in continuations
            if response.get('toxicity_score', 0) > 0.85
        ]

        if toxic_responses:
            # Append the most toxic response above the threshold
            top_response = max(toxic_responses, key=lambda x: x['Toxicity Score'])
            rows.append(top_response)

    return pd.DataFrame(rows)

# Combine top toxic responses from all files into a single DataFrame
top_toxic_responses_df = pd.concat(
    [get_top_toxic_responses(file_path) for file_path in file_paths],
    ignore_index=True
)

# Save the DataFrame to a CSV file
output_csv_path = '/content/top_toxic_responses.csv'
top_toxic_responses_df.to_csv(output_csv_path, index=False)

print(f"Top toxic responses saved to {output_csv_path}")



import json
import pandas as pd
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from captum.attr import FeatureAblation, LLMAttribution, TextTokenInput

# Load the Bloom model and tokenizer
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "bigscience/bloom-7b1"

# Create BitsAndBytesConfig for quantization
def create_bnb_config():
    return BitsAndBytesConfig(
        load_in_8bit=True,            # Enable 8-bit quantization
        llm_int8_threshold=6.0,      # Custom threshold for int8
        llm_int8_skip_modules=None,  # Set modules to skip for int8
        min_memory_reserved=1e9,     # Reserve memory for large models
        max_memory_reserved=1e10     # Max memory reserved
    )

# Load model and tokenizer
def load_model(model_name, bnb_config):
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",  # Automatically map layers to available GPUs
        quantization_config=bnb_config,  # Apply quantization config
        use_auth_token=False  # Explicitly disable token requirement
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=False)  # No token
    return model, tokenizer

# Instantiate bnb_config and load model/tokenizer
bnb_config = create_bnb_config()
model, tokenizer = load_model(model_name, bnb_config)
model.eval()

# Print success message
print("Model and tokenizer loaded successfully without a Hugging Face token!")

# Initialize Captum modules
fa = FeatureAblation(model)
llm_attr = LLMAttribution(fa, tokenizer)

# Function to filter prompts by LLM
def filter_prompts_by_llm(df, llm_name, num_prompts=10):
    filtered_df = df[df["File Path"].str.contains(llm_name, case=False)]
    return filtered_df.head(num_prompts)

# Combine top prompts from each LLM
def select_top_prompts(df, llm_names, num_prompts=10):
    selected_df = pd.concat([filter_prompts_by_llm(df, llm_name, num_prompts) for llm_name in llm_names], ignore_index=True)
    return selected_df

# Compute attributions for each prompt-response pair in DataFrame
def compute_attributions_for_df(df):
    results = []
    for _, row in df.iterrows():
        prompt = row["Prompt"]
        response = row["Response"]
        print(f"Processing Prompt: {prompt}")

        # Prepare inputs for attribution
        skip_tokens = [1]  # Skip special token like <s>
        inp = TextTokenInput(prompt, tokenizer, skip_tokens=skip_tokens)

        # Perform attribution
        try:
            attr_res = llm_attr.attribute(inp, target=response, skip_tokens=skip_tokens)

            # Collect results
            results.append({
                "Prompt": prompt,
                "Response": response,
                "Input Token Attributions": attr_res.seq_attr.tolist(),
                "Output Token Attributions": attr_res.token_attr.tolist()
            })
        except Exception as e:
            print(f"Error processing prompt: {prompt}, response: {response}")
            print(str(e))

    return pd.DataFrame(results)

# Define LLM names based on file paths in the DataFrame
llm_names = ["bloom", "gpt2", "mistral", "llama"]

# Load top_toxic_responses_df (replace this with the actual loading mechanism)
file_paths = [
    '/content/bloom7b_eval_k50_p1_t1.0_n25_l50 (1).json',
    '/content/gpt2_eval_k50_p1_t1.0_n25_l150.json',
    '/content/mistral_eval_k50_p1_t1.0_n25_l150.json',
    '/content/llama_eval_k50_p1_t1.0_n25_l50.json'
]

def get_top_toxic_responses(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)

    rows = []

    for prompt in data.get('generated_responses', []):
        prompt_text = prompt.get('prompt', {}).get('text', '')
        continuations = prompt.get('continuations', [])

        # Find the response with the highest toxicity score for this prompt above the threshold
        toxic_responses = [
            {
                'Prompt': prompt_text,
                'Response': response.get('text', ''),
                'Toxicity Score': response.get('toxicity_score', 0),
                'File Path': file_path
            }
            for response in continuations
            if response.get('toxicity_score', 0) > 0.85
        ]

        if toxic_responses:
            # Append the most toxic response above the threshold
            top_response = max(toxic_responses, key=lambda x: x['Toxicity Score'])
            rows.append(top_response)

    return pd.DataFrame(rows)

# Combine top toxic responses from all files into a single DataFrame
top_toxic_responses_df = pd.concat(
    [get_top_toxic_responses(file_path) for file_path in file_paths],
    ignore_index=True
)

# Select top 10 prompts for each LLM
selected_prompts_df = select_top_prompts(top_toxic_responses_df, llm_names, num_prompts=10)

# Compute attributions for selected prompts
attribution_results_df = compute_attributions_for_df(selected_prompts_df)

# Save the attribution results
output_attribution_csv_path = "/content/attribution_results_filtered.csv"
attribution_results_df.to_csv(output_attribution_csv_path, index=False)

print(f"Attribution results saved to {output_attribution_csv_path}")



selected_prompts_df


import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Helper function to visualize token-level attributions
def plot_token_attributions(input_tokens, attributions, title="Token Attribution Visualization"):
    plt.figure(figsize=(12, 6))
    bar_colors = ["blue" if score > 0 else "red" for score in attributions]

    # Plot bar chart of token attributions
    plt.bar(range(len(input_tokens)), attributions, color=bar_colors, alpha=0.7)
    plt.xticks(range(len(input_tokens)), input_tokens, rotation=90)
    plt.xlabel("Input Tokens")
    plt.ylabel("Attribution Score")
    plt.title(title)
    plt.tight_layout()
    plt.show()

# Helper function to visualize attribution heatmap
def plot_attribution_heatmap(input_tokens, output_tokens, attributions, title="Attribution Heatmap"):
    plt.figure(figsize=(14, 8))
    sns.heatmap(
        attributions,
        annot=False,
        xticklabels=input_tokens,
        yticklabels=output_tokens,
        cmap="coolwarm",
        cbar_kws={"label": "Attribution Score"}
    )
    plt.xlabel("Input Tokens")
    plt.ylabel("Output Tokens")
    plt.title(title)
    plt.tight_layout()
    plt.show()

# Generate visualizations for a specific row in the attribution results DataFrame
def visualize_attributions(row):
    input_tokens = tokenizer.tokenize(row["Prompt"])
    output_tokens = tokenizer.tokenize(row["Response"])
    input_attributions = row["Input Token Attributions"]
    output_attributions = row["Output Token Attributions"]

    print(f"Visualizing for Prompt: {row['Prompt']}")
    print(f"Response: {row['Response']}")

    # Token-level attribution visualization
    plot_token_attributions(input_tokens, input_attributions, title="Input Token Attributions")

    # Attribution heatmap
    plot_attribution_heatmap(
        input_tokens,
        output_tokens,
        output_attributions,
        title="Input-Output Attribution Heatmap"
    )

# Example usage: Visualize attributions for the first row
for idx, row in attribution_results_df.iterrows():
    visualize_attributions(row)
    if idx == 10:  # Visualize only a few examples
        break






# Function to plot most influential tokens for a specific row in the DataFrame
def plot_most_influential_tokens_from_df(df, row_index, k=5, title="Most Influential Tokens"):
    # Extract the specific row
    row = df.iloc[row_index]

    # Tokenize the prompt and extract attributions
    input_tokens = tokenizer.tokenize(row["Prompt"])
    input_attributions = row["Input Token Attributions"]

    # Sort and select the top-k influential tokens
    influential_tokens = sorted(zip(input_tokens, input_attributions), key=lambda x: abs(x[1]), reverse=True)[:k]
    tokens, scores = zip(*influential_tokens)

    # Plot the bar chart
    plt.figure(figsize=(8, 6))
    plt.bar(tokens, scores, color="skyblue")
    plt.xlabel("Tokens")
    plt.ylabel("Attribution Score")
    plt.title(title)
    plt.tight_layout()
    plt.show()

# Example usage for the first row in the DataFrame
plot_most_influential_tokens_from_df(attribution_results_df, row_index=0, k=5)






print(attribution_results_df.columns)



import json
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
from captum.attr import FeatureAblation, LLMAttribution, TextTokenInput

# Define Hugging Face token
hf_token = "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"

# Define Mistral model details
device = "cuda" if torch.cuda.is_available() else "cpu"
mistral_model_name = "mistralai/Mistral-7B-v0.1"  # Correct repository name

# Load Mistral model and tokenizer
def load_mistral_model(model_name, token):
    """Load the Mistral model and tokenizer."""
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",  # Automatically map layers to available GPUs
        use_auth_token=token  # Pass Hugging Face token for authentication
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)
    return model, tokenizer

mistral_model, mistral_tokenizer = load_mistral_model(mistral_model_name, hf_token)
mistral_model.eval()

print("Mistral model and tokenizer loaded successfully!")

# Function to filter rows for Mistral
def filter_rows_for_mistral(df, start_index=20, end_index=30):
    filtered_df = df[
        df["File Path"].str.contains("mistral", case=False, na=False)
    ].iloc[start_index:end_index]
    return filtered_df

# Initialize Captum modules
fa = FeatureAblation(mistral_model)
llm_attr = LLMAttribution(fa, mistral_tokenizer)

# Compute attributions
def compute_attributions_for_df(df):
    results = []
    for _, row in df.iterrows():
        prompt = row["Prompt"]
        response = row["Response"]
        print(f"Processing Prompt: {prompt}")

        # Prepare inputs for attribution
        skip_tokens = [1]  # Skip special tokens like <s>
        inp = TextTokenInput(prompt, mistral_tokenizer, skip_tokens=skip_tokens)

        # Perform attribution
        try:
            attr_res = llm_attr.attribute(inp, target=response, skip_tokens=skip_tokens)
            print(len(attr_res.input_tokens))
            print(len(attr_res.output_tokens))
            
            # Collect results
            results.append({
                "Prompt": prompt,
                "Response": response,
                "Input Token Attributions": attr_res.seq_attr.tolist(),
                "Output Token Attributions": attr_res.token_attr.tolist()
            })
        except Exception as e:
            print(f"Error processing prompt: {prompt}, response: {response}")
            print(str(e))

    return pd.DataFrame(results)

# Load the CSV and filter rows for Mistral
input_csv_path = "/content/top_toxic_responses.csv"
attribution_results_df = pd.read_csv(input_csv_path)

# Filter rows for indices 20–29
mistral_subset_df = filter_rows_for_mistral(attribution_results_df, start_index=20, end_index=30)

# Compute attributions for the filtered subset
mistral_attribution_results_df = compute_attributions_for_df(mistral_subset_df)

# Save the attribution results
output_csv_path = "/content/attribution_results_mistral_20_29.csv"
mistral_attribution_results_df.to_csv(output_csv_path, index=False)

print(f"Mistral attribution results saved to {output_csv_path}")

