


!python -m spacy download en_core_web_sm huggingface_hub transformers

import pandas as pd
import ast 
import numpy as np
from huggingface_hub import login
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import spacy
from scipy.stats import spearmanr
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
import ast
import re





# load tokenizers and get tokens for all the prompts
login("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")
llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")

login("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")
mistral_tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1") 

bloom_tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-7b1")

gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")

tokenizers = {'Llama':llama_tokenizer,
              "Bloom":bloom_tokenizer,
            "GPT2":gpt2_tokenizer}





def read_attribution_scores(file_path):
    def parse_array_or_matrix(value):
        try:
            # print(ast.literal_eval(value))
            return np.array(ast.literal_eval(value), dtype=float) # Safely convert string representation to Python list
        except (ValueError, SyntaxError):
            # print("error")
            return value  # Return as-is if it cannot be parsed

    df = pd.read_csv(
        file_path,
        converters={
            'Input Token Attributions': parse_array_or_matrix,
            'Output Token Attributions': parse_array_or_matrix,
        }
    )

    return df

def read_all_attribution_scores(): 
    gpt2 = 'GPT2_data/gpt2_attribution_results.csv'
    bloom = 'Bloom/bloom_attribution_results.csv'
    llama = 'Llama/llama_attribution_results.csv'

    gpt2_df = read_attribution_scores(gpt2)
    gpt2_df['Model'] = 'GPT2'

    bloom_df = read_attribution_scores(bloom)
    bloom_df['Model'] = 'Bloom'

    llama_df = read_attribution_scores(llama)
    llama_df['Model'] = 'Llama'

    combined_df = pd.concat([llama_df, gpt2_df, bloom_df], ignore_index=True)

    return combined_df    


df = read_all_attribution_scores()
df.head()





def map_attributions_to_tokens(row, top_percentile_general=80, top_percentile_to_output=90):
    # Tokenize the prompt and response
    og_tokenized_prompt = eval(row["Input Tokens"])
    og_tokenized_response = eval(row["Output Tokens"])

    # clean the tokens for readability
    tokenized_prompt = [tokenizers[row['Model']].convert_tokens_to_string([token]).strip() for token in og_tokenized_prompt]
    tokenized_response = [tokenizers[row['Model']].convert_tokens_to_string([token]).strip() for token in og_tokenized_response]

    input_attributions = row["Input Token Attributions"]
    output_attributions = row["Output Token Attributions"].T  # transpose the matrix (now: rows correspond to input tokens and column corresponds to response tokens)

    # handle mismatched stuff   
    # if row["Model"] == "Llama" and len(tokenized_prompt) < output_attributions.shape[0]:
    #     output_attributions = output_attributions[:-1]
    #     input_attributions = input_attributions[:-1]
        
    # if row["Model"] == "Bloom" and len(tokenized_response) > output_attributions.shape[1]:
    #     tokenized_response = tokenized_response[:-1]

    # if row["Model"] == "Llama":
    # print(row["Model"])
    # print("tokenized prompt: ", len(tokenized_prompt))
    # print("input attribution: ", len(input_attributions))
    # print("tokenized response: ", len(tokenized_response))
    # print("attributions of inputs wrt outputs: ", output_attributions.shape)

    #     for i in range(len(eval(row["Output Tokens"]))):
    #         print(eval(row["Output Tokens"])[i])

    if len(tokenized_prompt) != len(input_attributions):
        raise ValueError(f"Mismatch between tokenized prompt and input attributions for row:\n{row}")
    if output_attributions.shape[0] != len(tokenized_prompt) or output_attributions.shape[1] != len(tokenized_response):
        raise ValueError(f"Mismatch in output attribution matrix dimensions for row:\n{row}")
    
    # ----------------------------------------------------------------------------
    # PART A: High Input Tokens (General) using relative threshold
    # ----------------------------------------------------------------------------
    general_cutoff = np.percentile(input_attributions, top_percentile_general)

    high_input_tokens_general = [
        (tokenized_prompt[i], input_attributions[i])
        for i, score in enumerate(input_attributions) if score > general_cutoff
    ]

    # ----------------------------------------------------------------------------
    # PART B: High Input Tokens (Column Max)
    # ----------------------------------------------------------------------------
    num_input_tokens, num_output_tokens = output_attributions.shape
    max_influence_per_column = []        # max attribution value for each column
    max_influence_index_per_column = []  # which input token index had that max value

    for j in range(num_output_tokens):
        col_vals = output_attributions[:, j]
        max_i = np.argmax(col_vals)      # index of max attribution for this column
        max_val = col_vals[max_i]        # max attribution value
        max_influence_per_column.append(max_val)
        max_influence_index_per_column.append(max_i)

    col_max_cutoff = np.percentile(max_influence_per_column, top_percentile_to_output)

    high_input_tokens_wrt_output_indices = set()
    for j, max_val in enumerate(max_influence_per_column):
        if max_val > col_max_cutoff:
            max_i = max_influence_index_per_column[j]
            high_input_tokens_wrt_output_indices.add(max_i)

    high_input_tokens_wrt_output = [
        (tokenized_prompt[i], max(
            output_attributions[i, j] for j in range(num_output_tokens) if max_influence_index_per_column[j] == i
        )) for i in sorted(high_input_tokens_wrt_output_indices)
    ]

    # print(tokenized_prompt)
    # print(high_input_tokens_general)
    # print(high_input_tokens_wrt_output)
    
    return {
        "Prompt Text": row['Prompt'],
        "Tokenized Prompt": tokenized_prompt,
        "High Input Tokens General": [t[0] for t in high_input_tokens_general],
        "High Input Tokens General Attributions": [t[1] for t in high_input_tokens_general],
        "High Input Tokens To Output": [t[0] for t in high_input_tokens_wrt_output],
        "High Input Tokens To Output Attributions": [t[1] for t in high_input_tokens_wrt_output]
    }





import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

def plot_attributions_from_df(df, row_index, 
                              line_color='red', 
                              min_opacity=0.2, 
                              max_opacity=1.0,
                              line_width=2.0,
                              threshold=0.1, 
                              max_input_tokens=50, 
                              max_output_tokens=50):
    """
    Plots token attributions for a specific row in the DataFrame, limiting input and output tokens.
    The line opacity is based on attribution strength for better clarity.
    """
    row = df.iloc[row_index]

    prompt = row['Prompt'].split()
    response = row['Response'].split()
    input_tokens = eval(row['Input Tokens'])  
    output_tokens = eval(row['Output Tokens'])
    attributions = np.array(row['Output Token Attributions']).T  # Transpose for correct shape

    print(f"Total Input Tokens: {len(input_tokens)}")
    print(f"Total Output Tokens: {len(output_tokens)}")
    print(f"Attributions Shape: {attributions.shape}")
    
    if attributions.shape != (len(input_tokens), len(output_tokens)):
        raise ValueError("Mismatch between tokens and attribution dimensions.")

    # Normalize attributions for opacity
    flat_attrs = attributions.flatten()
    min_val, max_val = flat_attrs.min(), flat_attrs.max()
    denom = (max_val - min_val) if max_val != min_val else 1e-9

    # Filter top input tokens based on attribution scores
    input_scores = attributions.max(axis=1)  # Max influence of each input token
    top_input_indices = np.argsort(input_scores)[-max_input_tokens:]  # Top input tokens by score

    # Filter top output tokens based on attribution scores
    output_scores = attributions.max(axis=0)  # Max influence of each output token
    top_output_indices = np.argsort(output_scores)[-max_output_tokens:]  # Top output tokens by score

    # Filter tokens and attributions
    input_tokens = [input_tokens[i] for i in top_input_indices]
    output_tokens = [output_tokens[j] for j in top_output_indices]
    attributions = attributions[np.ix_(top_input_indices, top_output_indices)]  # Filter rows and columns

    print(f"Filtered Input Tokens: {len(input_tokens)}")
    print(f"Filtered Output Tokens: {len(output_tokens)}")
    print(f"Filtered Attributions Shape: {attributions.shape}")

    x_left, x_right = 0, 1
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.set_facecolor('white')
    ax.set_xlim([-0.5, 1.5])
    ax.set_ylim([-1, max(len(input_tokens), len(output_tokens)) + 1])

    # Plot input tokens on the left
    for i, token in enumerate(input_tokens):
        ax.text(x_left - 0.1, i, token, ha='right', va='center', color='black', fontsize=10)

    # Plot output tokens on the right
    for j, token in enumerate(output_tokens):
        ax.text(x_right + 0.1, j, token, ha='left', va='center', color='black', fontsize=10)

    # Draw the connecting lines with opacity based on attribution scores
    for i in range(len(input_tokens)):
        for j in range(len(output_tokens)):
            score = attributions[i, j]
            if score >= threshold:
                norm_score = (score - min_val) / denom
                alpha = min_opacity + norm_score * (max_opacity - min_opacity)
                ax.plot([x_left, x_right], [i, j], color=line_color, alpha=alpha, linewidth=line_width)

    ax.axis('off')
    plt.tight_layout()
    plt.show()

plot_attributions_from_df(df, row_index=13, line_color='blue', min_opacity=0.0005, max_opacity=1.0, line_width=2.5, threshold=0.05, max_input_tokens=2, max_output_tokens=5)









nlp = spacy.load("en_core_web_sm")

def analyze_syntax(sentence):
    doc = nlp(sentence)
    syntax = []
    for token in doc:
        syntax.append({
            "Token": token.text,
            "POS": token.pos_,
            "Dependency": token.dep_,
            "Head": token.head.text,
            "Head POS": token.head.pos_,
            "Children": [(child.text, child.dep_, child.pos_) for child in token.children]
        })
    return syntax

def syntactic_analysis_with_tokens(df):
    """Result is analysis per prompt
    """
    results = []
    for _, row in df.iterrows():
        model = row["Model"]

        # these tokens are also filtered so that only the tokens with high attr.
        # scores (wrt their sentence) are selected
        mapped_tokens = map_attributions_to_tokens(row)
        
        # Perform syntactic analysis on the entire prompt
        prompt_text = mapped_tokens["Prompt Text"]
        prompt_syntax = analyze_syntax(prompt_text)

        # Use dictionaries to map tokens to their attribution scores
        high_input_tokens_dict = dict(zip(
            mapped_tokens["High Input Tokens General"],
            mapped_tokens["High Input Tokens General Attributions"]
        ))
        high_output_tokens_dict = dict(zip(
            mapped_tokens["High Input Tokens To Output"],
            mapped_tokens["High Input Tokens To Output Attributions"]
        ))

        # Analyze high-attribution tokens
        high_input_syntax = []
        high_output_syntax = []
        for token_data in prompt_syntax:
            token_text = token_data["Token"]
            if token_text in high_input_tokens_dict:
                # Add attribution score to the token_data
                token_data["Attribution Score (General)"] = high_input_tokens_dict[token_text]
                high_input_syntax.append(token_data)
            if token_text in high_output_tokens_dict:
                # Add attribution score to the token_data
                token_data["Attribution Score (To Output)"] = high_output_tokens_dict[token_text]
                high_output_syntax.append(token_data)

        # Append results
        results.append({
            "Prompt": row["Prompt"],
            "Prompt Syntax": prompt_syntax,
            "High General Input Tokens Syntax": high_input_syntax,
            "High Influential Input Tokens Syntax": high_output_syntax,
            "Model": row["Model"]
        })
    return pd.DataFrame(results)


from collections import defaultdict

def create_token_dependency_df(syntactic_results):
    """
    For each (token, dependency) combination across all prompts,
    produce:
      - how many times that token had that dependency
      - average attribution score (General) when it's in that dependency
      - average attribution score (To Output) when it's in that dependency
    """

    # aggregator structure keyed by (token_text, dependency_label)
    # each entry will keep track of counts & sum of attributions
    aggregator = defaultdict(lambda: {
        "count": 0,
        "sum_general": 0.0,
        "sum_to_output": 0.0
    })

    # Go through each row in syntactic_results
    for idx, row in syntactic_results.iterrows():
        prompt_syntax = row["Prompt Syntax"]  # list of dicts

        # Each dict in prompt_syntax has:
        # {
        #   "Token": str,
        #   "POS": str,
        #   "Dependency": str,
        #   "Head": str,
        #   "Head POS": str,
        #   "Children": list of (child_text, child_dep, child_pos),
        #   "Attribution Score (General)": float (optional),
        #   "Attribution Score (To Output)": float (optional)
        # }
        for token_data in prompt_syntax:
            token_text = token_data["Token"]
            dep_label = token_data["Dependency"]

            # Update our aggregator counts
            aggregator[(token_text, dep_label)]["count"] += 1

            # If the token has an attribution score, add it
            if "Attribution Score (General)" in token_data:
                aggregator[(token_text, dep_label)]["sum_general"] += token_data["Attribution Score (General)"]
            if "Attribution Score (To Output)" in token_data:
                aggregator[(token_text, dep_label)]["sum_to_output"] += token_data["Attribution Score (To Output)"]
    
    # Convert the aggregator dict into a DataFrame
    rows = []
    for (token, dep), vals in aggregator.items():
        count = vals["count"]
        avg_general = vals["sum_general"] / count if count else 0
        avg_output = vals["sum_to_output"] / count if count else 0
        
        rows.append({
            "Token": token,
            "Dependency": dep,
            "Count": count,
            "Average Attribution (General)": avg_general,
            "Average Attribution (To Output)": avg_output
        })
    
    return pd.DataFrame(rows)





syntactic_results = syntactic_analysis_with_tokens(df)

# print(syntactic_results.head())
print(syntactic_results.info())

token_dependency_df = create_token_dependency_df(syntactic_results)
print(token_dependency_df.head(10))








position_of_prompt = 0


# Example: Inspecting a single prompt's syntax
example_prompt_syntax = syntactic_results.iloc[position_of_prompt]["Prompt Syntax"]

# Convert to DataFrame for better readability
prompt_syntax_df = pd.DataFrame(example_prompt_syntax)
print(prompt_syntax_df)


# Inspect high-attribution input tokens for a specific row
example_high_input_syntax = syntactic_results.iloc[position_of_prompt]["High General Input Tokens Syntax"]
print(pd.DataFrame(example_high_input_syntax))


# Inspect high-attribution output tokens for a specific row
example_high_output_syntax = syntactic_results.iloc[position_of_prompt]["High Influential Input Tokens Syntax"]
print(pd.DataFrame(example_high_output_syntax))


from spacy import displacy

# Visualize the syntax tree for the first prompt
doc = nlp(syntactic_results.iloc[position_of_prompt]["Prompt"])
displacy.render(doc, style="dep", jupyter=True)








import matplotlib.pyplot as plt

# Dictionary to store aggregated data for all models
all_models_data = {}

# Collect POS frequencies for each model
for model in syntactic_results["Model"].unique():
    model_data = syntactic_results[syntactic_results["Model"] == model]
    
    # Aggregate POS tags for high-input tokens
    pos_counts = pd.Series(
        [entry["POS"] for syntax in model_data["High Influential Input Tokens Syntax"] for entry in syntax]
    ).value_counts()
    
    # Map POS tags to explanations
    pos_explanations = pos_counts.index.map(spacy.explain)
    
    # Add the data to the dictionary
    all_models_data[model] = pd.DataFrame({
        "POS": pos_counts.index,
        "Frequency": pos_counts.values,
        "Explanation": pos_explanations
    })

# Create a unified set of POS explanations (for x-axis consistency)
all_explanations = set()
for data in all_models_data.values():
    all_explanations.update(data["Explanation"])

all_explanations = sorted(all_explanations)  # Sort for consistent x-axis

# Initialize a plot
plt.figure(figsize=(12, 8))

plt.figure(figsize=(12, 8))
for model, data in all_models_data.items():
    aligned_frequencies = [
        data.loc[data["Explanation"] == explanation, "Frequency"].sum() if explanation in data["Explanation"].values else 0
        for explanation in all_explanations
    ]
    plt.plot(all_explanations, aligned_frequencies, label=model, marker='o')

plt.title("POS Distribution Across Models")
plt.xlabel("Part of Speech Explanation")
plt.ylabel("Frequency")
plt.xticks(rotation=45, ha="right")
plt.legend(title="Models")
plt.tight_layout()
plt.show()





import matplotlib.pyplot as plt

# Dictionary to store aggregated data for all models
all_models_data = {}

# Collect POS frequencies for each model
for model in syntactic_results["Model"].unique():
    model_data = syntactic_results[syntactic_results["Model"] == model]
    
    # Aggregate POS tags for high-input tokens
    pos_counts = pd.Series(
        [entry["POS"] for syntax in model_data["High General Input Tokens Syntax"] for entry in syntax]
    ).value_counts()
    
    # Map POS tags to explanations
    pos_explanations = pos_counts.index.map(spacy.explain)
    
    # Add the data to the dictionary
    all_models_data[model] = pd.DataFrame({
        "POS": pos_counts.index,
        "Frequency": pos_counts.values,
        "Explanation": pos_explanations
    })

# Create a unified set of POS explanations (for x-axis consistency)
all_explanations = set()
for data in all_models_data.values():
    all_explanations.update(data["Explanation"])

all_explanations = sorted(all_explanations)  # Sort for consistent x-axis

# Initialize a plot
plt.figure(figsize=(12, 8))

plt.figure(figsize=(12, 8))
for model, data in all_models_data.items():
    aligned_frequencies = [
        data.loc[data["Explanation"] == explanation, "Frequency"].sum() if explanation in data["Explanation"].values else 0
        for explanation in all_explanations
    ]
    plt.plot(all_explanations, aligned_frequencies, label=model, marker='o')

plt.title("POS Distribution Across Models")
plt.xlabel("Part of Speech Explanation")
plt.ylabel("Frequency")
plt.xticks(rotation=45, ha="right")
plt.legend(title="Models")
plt.tight_layout()
plt.show()





# POS frequency for high-input tokens
pos_counts_input = pd.Series(
    [entry["POS"] for syntax in syntactic_results["High General Input Tokens Syntax"] for entry in syntax]
).value_counts()

# Create a DataFrame for high-input tokens with explanations
pos_df_input = pd.DataFrame({
    "POS": pos_counts_input.index,
    "Frequency": pos_counts_input.values,
    "Explanation": pos_counts_input.index.map(spacy.explain)
})

print("POS Frequency and Explanation for High Input Tokens:\n", pos_df_input)
print("----------------------------------------------------------------------------")

# POS frequency for high-output tokens
pos_counts_output = pd.Series(
    [entry["POS"] for syntax in syntactic_results["High Influential Input Tokens Syntax"] for entry in syntax]
).value_counts()

# Create a DataFrame for high-output tokens with explanations
pos_df_output = pd.DataFrame({
    "POS": pos_counts_output.index,
    "Frequency": pos_counts_output.values,
    "Explanation": pos_counts_output.index.map(spacy.explain)
})

print("POS Frequency and Explanation for High Output Tokens:\n", pos_df_output)


import matplotlib.pyplot as plt
import numpy as np

# Align both dataframes to ensure consistency in x-axis (all explanations included)
all_explanations = sorted(set(pos_df_input["Explanation"]).union(set(pos_df_output["Explanation"])))

# Align frequencies to the unified list of explanations (fill missing values with 0)
input_frequencies = [
    pos_df_input.loc[pos_df_input["Explanation"] == explanation, "Frequency"].sum() if explanation in pos_df_input["Explanation"].values else 0
    for explanation in all_explanations
]
output_frequencies = [
    pos_df_output.loc[pos_df_output["Explanation"] == explanation, "Frequency"].sum() if explanation in pos_df_output["Explanation"].values else 0
    for explanation in all_explanations
]

# Combine data into a DataFrame for sorting
data = list(zip(all_explanations, input_frequencies, output_frequencies))
sorted_data = sorted(data, key=lambda x: x[1] + x[2], reverse=True)  # Sort by total frequency (descending)

# Unpack sorted data
sorted_explanations, sorted_input_frequencies, sorted_output_frequencies = zip(*sorted_data)

# Set bar positions
x = np.arange(len(sorted_explanations))  # X positions for the bars
width = 0.4  # Width of the bars

# Plot the bars
plt.figure(figsize=(12, 8))
plt.bar(x - width / 2, sorted_input_frequencies, width, label="General Input Tokens")
plt.bar(x + width / 2, sorted_output_frequencies, width, label="Influential Input Tokens")

# Add labels, title, and legend
plt.title("POS Distribution Comparison (Sorted by Total Frequency)")
plt.xlabel("Part of Speech")
plt.ylabel("Frequency")
plt.xticks(x, sorted_explanations, rotation=45, ha="right")
plt.legend(title="Token Type")
plt.tight_layout()
plt.show()





all_models_data = {}

# Collect dependency label frequencies for each model
for model in syntactic_results["Model"].unique():
    model_data = syntactic_results[syntactic_results["Model"] == model]
    
    # Aggregate dependency labels for high-input tokens
    dep_counts = pd.Series(
        [entry["Dependency"] for syntax in model_data["High General Input Tokens Syntax"] for entry in syntax]
    ).value_counts()
    
    # Map dependency labels to explanations using spacy.explain
    dep_explanations = dep_counts.index.map(spacy.explain)
    
    # Add the data to the dictionary
    all_models_data[model] = pd.DataFrame({
        "Dependency": dep_counts.index,
        "Frequency": dep_counts.values,
        "Explanation": dep_explanations
    })

# Create a unified set of dependency explanations (for consistent x-axis)
all_explanations = set()
for data in all_models_data.values():
    all_explanations.update(data["Explanation"])

all_explanations = sorted(all_explanations)  # Sort for consistent x-axis

# Initialize a plot
plt.figure(figsize=(12, 8))

# Plot each model's dependency label data
for model, data in all_models_data.items():
    aligned_frequencies = [
        data.loc[data["Explanation"] == explanation, "Frequency"].sum() if explanation in data["Explanation"].values else 0
        for explanation in all_explanations
    ]
    plt.plot(all_explanations, aligned_frequencies, label=model, marker='o')

plt.title("Dependency Label Distribution Across Models")
plt.xlabel("Dependency Roles")
plt.ylabel("Frequency")
plt.xticks(rotation=45, ha="right")
plt.legend(title="Models")
plt.tight_layout()
plt.show()





# Assume syntactic_results and High General/High Influential Input Tokens Syntax are predefined
dependency_counts_input = pd.Series(
    [entry["Dependency"] for syntax in syntactic_results["High General Input Tokens Syntax"] for entry in syntax]
).value_counts()

dependency_counts_output = pd.Series(
    [entry["Dependency"] for syntax in syntactic_results["High Influential Input Tokens Syntax"] for entry in syntax]
).value_counts()

# Create DataFrames for each
df_input = pd.DataFrame({
    "Dependency": dependency_counts_input.index,
    "Frequency": dependency_counts_input.values,
    "Explanation": dependency_counts_input.index.map(spacy.explain)
})

df_output = pd.DataFrame({
    "Dependency": dependency_counts_output.index,
    "Frequency": dependency_counts_output.values,
    "Explanation": dependency_counts_output.index.map(spacy.explain)
})

print("Dependency Frequency and Explanation for High Input Tokens:\n", df_input)
print("----------------------------------------------------------------------------")
print("Dependency Frequency and Explanation for High Input-to-Output Tokens:\n", df_output)


import matplotlib.pyplot as plt
import numpy as np

# Align both DataFrames to ensure consistent x-axis (all dependencies included)
all_dependencies = sorted(set(df_input["Dependency"]).union(set(df_output["Dependency"])))

# Align frequencies to the unified list of dependencies (fill missing values with 0)
input_frequencies = [
    df_input.loc[df_input["Dependency"] == dep, "Frequency"].sum() if dep in df_input["Dependency"].values else 0
    for dep in all_dependencies
]
output_frequencies = [
    df_output.loc[df_output["Dependency"] == dep, "Frequency"].sum() if dep in df_output["Dependency"].values else 0
    for dep in all_dependencies
]

# Combine data into a list for sorting
data = list(zip(all_dependencies, input_frequencies, output_frequencies))

# Sort data by the total frequency (input + output) in descending order
sorted_data = sorted(data, key=lambda x: x[1] + x[2], reverse=True)

# Unpack sorted data
sorted_dependencies, sorted_input_frequencies, sorted_output_frequencies = zip(*sorted_data)

# Get corresponding explanations for the sorted dependencies
sorted_explanations = [
    df_input.loc[df_input["Dependency"] == dep, "Explanation"].values[0] if dep in df_input["Dependency"].values else
    df_output.loc[df_output["Dependency"] == dep, "Explanation"].values[0]
    for dep in sorted_dependencies
]

# Set bar positions
x = np.arange(len(sorted_dependencies))  # X positions for the bars
width = 0.4  # Width of the bars

# Plot the bars
plt.figure(figsize=(12, 8))
plt.bar(x - width / 2, sorted_input_frequencies, width, label="High General Input Tokens")
plt.bar(x + width / 2, sorted_output_frequencies, width, label="High Influential Input Tokens")

# Add labels, title, and legend
plt.title("Dependency Distribution Comparison")
plt.xlabel("Dependency Roles")
plt.ylabel("Frequency")
plt.xticks(x, sorted_explanations, rotation=45, ha="right")
plt.legend(title="Token Type")
plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()





import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from collections import Counter
import spacy

def load_ud_data():
    """Load UD EWT data from file"""
    with open("./UD_English-EWT/en_ewt-ud-train.conllu", "r", encoding="utf-8") as file:
        lines = file.readlines()
    return [line.split("\t")[7] for line in lines if not line.startswith("#") and line.strip()]

def process_data(syntactic_results):
    """Process your existing data into normalized DataFrames"""
    # Create Series from your existing code
    dependency_counts_input = pd.Series(
        [entry["Dependency"] for syntax in syntactic_results["High General Input Tokens Syntax"] 
         for entry in syntax]
    ).value_counts()
    
    dependency_counts_output = pd.Series(
        [entry["Dependency"] for syntax in syntactic_results["High Influential Input Tokens Syntax"] 
         for entry in syntax]
    ).value_counts()

    # Create DataFrames with human-readable explanations
    def create_df(series, label):
        df = pd.DataFrame({
            "Dependency": series.index,
            "Frequency": series.values,
            "Explanation": series.index.map(lambda x: spacy.explain(x.split(':')[0]) or x)
        })
        df["Normalized"] = df["Frequency"] / df["Frequency"].sum() * 100
        df["Label"] = label
        return df

    return (
        create_df(dependency_counts_input, "High General"),
        create_df(dependency_counts_output, "High Influential")
    )

def plot_smooth_comparison(ud_data, df_general, df_influential, top_n=15):
    """Create focused comparison plot for influential dependencies"""
    # Get influential dependencies
    influential_deps = df_influential["Dependency"].unique()
    
    # Filter all datasets
    ud_filtered = ud_data[ud_data["Dependency"].isin(influential_deps)]
    gen_filtered = df_general[df_general["Dependency"].isin(influential_deps)]
    inf_filtered = df_influential.copy()

    # Normalize frequencies
    def normalize(df):
        total = df["Frequency"].sum()
        return df.assign(Normalized=df["Frequency"] / total * 100)

    ud_norm = normalize(ud_filtered)
    gen_norm = normalize(gen_filtered)
    inf_norm = normalize(inf_filtered)

    # Merge data
    merged = (
        inf_norm[["Dependency", "Normalized"]]
        .merge(gen_norm[["Dependency", "Normalized"]], on="Dependency", suffixes=('_inf', '_gen'))
        .merge(ud_norm[["Dependency", "Normalized"]], on="Dependency")
        .rename(columns={"Normalized": "Normalized_ud"})
    )

    # Sort by influential frequency
    merged = merged.sort_values("Normalized_inf", ascending=False).head(top_n)

    # Create explanations
    merged["Explanation"] = merged["Dependency"].apply(
        lambda x: spacy.explain(x.split(':')[0]) or x
    )

    # Plot configuration
    plt.figure(figsize=(12, 8))
    y = np.arange(len(merged))
    width = 0.25

    plt.barh(y + width, merged["Normalized_ud"], width, 
             label="General English", color="#4c72b0", alpha=0.9)
    plt.barh(y, merged["Normalized_gen"], width,
             label="High General", color="#dd8452", alpha=0.9)
    plt.barh(y - width, merged["Normalized_inf"], width,
             label="High Influential", color="#55a868", alpha=0.9)

    plt.title('Top {} Influential Dependency Roles\nNormalized Distribution Comparison'.format(top_n), 
             fontsize=14, pad=20)
    plt.xlabel('Normalized Frequency (%)', fontsize=12)
    plt.ylabel('Dependency Roles', fontsize=12)
    plt.yticks(y, merged["Explanation"], fontsize=10)
    plt.gca().invert_yaxis()
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    plt.legend(bbox_to_anchor=(1, 1), frameon=False)
    plt.tight_layout()
    plt.show()


# Usage flow:
# 1. Load your syntactic_results data
# 2. Process data
ud_data = load_ud_data()
df_general, df_influential = process_data(syntactic_results)

# 3. Generate plot
plot_comparison(ud_data, df_general, df_influential)





def get_dependency_roles():
    """Retrieve all dependency roles available in SpaCy."""
    nlp = spacy.load("en_core_web_sm")
    return list(nlp.get_pipe("parser").labels)

roles = get_dependency_roles()
print(roles)
print(spacy.explain(roles[2]))


def compute_weighted_influence_by_role(syntactic_results_df):
    import pandas as pd
    import numpy as np

    dependency_roles = get_dependency_roles()

    # Initialize a dictionary to store scores for each dependency role
    role_scores = {role: {"High General Input Tokens": [], "High Influential Input Tokens": []} for role in dependency_roles}

    for _, row in syntactic_results_df.iterrows():
        prompt = row["Prompt"]
        model = row["Model"]

        # Extract high-attribution tokens with syntax info
        general_input_syntax = row["High General Input Tokens Syntax"]
        influential_input_syntax = row["High Influential Input Tokens Syntax"]

        # Map attribution scores back to roles
        for token_data in general_input_syntax:
            token = token_data["Token"]
            dep_role = token_data["Dependency"]
            attr_score = token_data["Attribution Score (General)"]
            if dep_role in role_scores:
                role_scores[dep_role]["High General Input Tokens"].append(attr_score)

        for token_data in influential_input_syntax:
            token = token_data["Token"]
            dep_role = token_data["Dependency"]
            attr_score = token_data["Attribution Score (To Output)"]
            if dep_role in role_scores:
                role_scores[dep_role]["High Influential Input Tokens"].append(attr_score)

    # Convert role_scores to a DataFrame
    records = []
    for role, scores in role_scores.items():
        for token_type, values in scores.items():
            if values:  # Only consider roles with attribution scores
                records.append({
                    "Role": role,
                    "Type": token_type,
                    "AverageAttribution": np.mean(values),
                    "TokenCount": len(values)
                })

    role_scores_df = pd.DataFrame(records)
    return role_scores_df

# Example usage:
avg_role_scores = compute_weighted_influence_by_role(syntactic_results)

# Display results
print(avg_role_scores)



# Filter data for plotting
general_tokens = avg_role_scores[avg_role_scores['Type'] == "High General Input Tokens"]
influential_tokens = avg_role_scores[avg_role_scores['Type'] == "High Influential Input Tokens"]

# Sort data by Average Attribution in descending order
general_tokens_sorted = general_tokens.sort_values(by='AverageAttribution', ascending=False)
influential_tokens_sorted = influential_tokens.sort_values(by='AverageAttribution', ascending=False)

# Get full explanations for the dependency roles
general_tokens_sorted['Role Explanation'] = general_tokens_sorted['Role'].apply(lambda role: spacy.explain(role) or role)
influential_tokens_sorted['Role Explanation'] = influential_tokens_sorted['Role'].apply(lambda role: spacy.explain(role) or role)

# Combine the data for a unified plot
combined = pd.merge(
    general_tokens_sorted[['Role Explanation', 'AverageAttribution']],
    influential_tokens_sorted[['Role Explanation', 'AverageAttribution']],
    on='Role Explanation',
    how='outer',
    suffixes=('_General', '_Influential')
).fillna(0)

# Sort combined data by general attribution
combined = combined.sort_values(by='AverageAttribution_Influential', ascending=True)

# Plot
plt.figure(figsize=(10, 14))
bar_width = 0.4
y_positions = range(len(combined))

# Bar chart for general and influential tokens
plt.barh(
    y_positions,
    combined['AverageAttribution_General'],
    height=bar_width,
    color='blue',
    alpha=0.7,
    label='High General Input Tokens'
)
plt.barh(
    [y + bar_width for y in y_positions],
    combined['AverageAttribution_Influential'],
    height=bar_width,
    color='orange',
    alpha=0.7,
    label='High Influential Input Tokens'
)

# Add explanations as y-ticks
plt.yticks(
    [y + bar_width / 2 for y in y_positions],
    combined['Role Explanation'],
    fontsize=10
)

# Add labels and title
plt.xlabel('Average Attribution Score', fontsize=12)
plt.ylabel('Dependency Role', fontsize=12)
plt.title('Weighted Influence by Dependency Role (Sorted by Highest Influence)', fontsize=14)
plt.legend(fontsize=10)
plt.tight_layout()

plt.show()


general_tokens_sorted.head(100)





# Adding a minimum frequency threshold
min_frequency = 3

# Filter out tokens that appear fewer than the minimum frequency
filtered_token_dependency_df = token_dependency_df[token_dependency_df["Count"] >= min_frequency]

# Re-sort by Combined Attribution in descending order
top_tokens_filtered = filtered_token_dependency_df.sort_values(by="Average Attribution (General)", ascending=False).head(10)





# Count occurrences of each token
token_counts = filtered_token_dependency_df['Token'].value_counts()

# Keep tokens with â‰¥2 dependency roles, because 1 is insignificant
multi_dep_tokens = token_counts[token_counts >= 2].index
multi_dep_df = filtered_token_dependency_df[filtered_token_dependency_df['Token'].isin(multi_dep_tokens)]

# Calculate max-min difference per token
token_diff = (
    multi_dep_df
    .groupby("Token")["Average Attribution (General)"]
    .agg(Difference=lambda x: x.max() - x.min())
    .reset_index()
)

# Sort tokens by their difference
token_diff_sorted = token_diff.sort_values("Difference", ascending=False)

# Select top/bottom tokens
top_tokens = token_diff_sorted.head(10)["Token"].tolist()
bottom_tokens = token_diff_sorted.tail(10)["Token"].tolist()

# Filter the original data for plotting
top_data = filtered_token_dependency_df[filtered_token_dependency_df["Token"].isin(top_tokens)]
bottom_data = filtered_token_dependency_df[filtered_token_dependency_df["Token"].isin(bottom_tokens)]

print(top_data.head(5))
print("------------------------------------------------------------------------------")
print(bottom_data.head(5))


def find_example_sentence(token, dependency, syntactic_results):
    """Find an example sentence where the token appears in the given dependency role."""
    for _, row in syntactic_results.iterrows():
        for token_data in row["Prompt Syntax"]:
            if token_data["Token"] == token and token_data["Dependency"] == dependency:
                return row["Prompt"]
    return "No example found"

# Analyze top 5 tokens with LARGEST differences
top_analysis = []
for token in top_tokens[:8]:
    token_data = multi_dep_df[multi_dep_df["Token"] == token]
    max_row = token_data.loc[token_data["Average Attribution (General)"].idxmax()]
    min_row = token_data.loc[token_data["Average Attribution (General)"].idxmin()]
    
    top_analysis.append({
        "Token": token,
        "Max Dependency": max_row["Dependency"],
        "Max Score": max_row["Average Attribution (General)"],
        "Min Dependency": min_row["Dependency"],
        "Min Score": min_row["Average Attribution (General)"],
        "Max Example": find_example_sentence(token, max_row["Dependency"], syntactic_results),
        "Min Example": find_example_sentence(token, min_row["Dependency"], syntactic_results)
    })

# Analyze top 5 tokens with SMALLEST differences
bottom_analysis = []
for token in bottom_tokens[:8]:
    token_data = multi_dep_df[multi_dep_df["Token"] == token]
    max_row = token_data.loc[token_data["Average Attribution (General)"].idxmax()]
    min_row = token_data.loc[token_data["Average Attribution (General)"].idxmin()]
    
    bottom_analysis.append({
        "Token": token,
        "Max Dependency": max_row["Dependency"],
        "Max Score": max_row["Average Attribution (General)"],
        "Min Dependency": min_row["Dependency"],
        "Min Score": min_row["Average Attribution (General)"],
        "Max Example": find_example_sentence(token, max_row["Dependency"], syntactic_results),
        "Min Example": find_example_sentence(token, min_row["Dependency"], syntactic_results)
    })

# Convert to DataFrames for display
top_analysis_df = pd.DataFrame(top_analysis)
bottom_analysis_df = pd.DataFrame(bottom_analysis)

print("=== Tokens with LARGEST Attribution Differences ===")
print(top_analysis_df[["Token", "Max Dependency", "Max Score", "Min Dependency", "Min Score", "Max Example", "Min Example"]])

print("\n=== Tokens with SMALLEST Attribution Differences ===")
print(bottom_analysis_df[["Token", "Max Dependency", "Max Score", "Min Dependency", "Min Score", "Max Example", "Min Example"]])


row = top_analysis_df.iloc[0]
print("max")
print(row['Token'])
print(row['Max Dependency'])
print(row['Max Example'])

print("\nmin")
print(row['Token'])
print(row['Min Dependency'])
print(row['Min Example'])



