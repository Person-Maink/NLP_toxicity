import pandas as pd
import ast 
import numpy as np
from huggingface_hub import login
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import spacy
from scipy.stats import spearmanr
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

!python -m spacy download en_core_web_sm



def read_attribution_scores(file_path):
    def parse_array_or_matrix(value):
        try:
            # print("trying")
            # print(ast.literal_eval(value))
            return np.array(ast.literal_eval(value), dtype=float) # Safely convert string representation to Python list
        except (ValueError, SyntaxError):
            # print("error")
            return value  # Return as-is if it cannot be parsed

    df = pd.read_csv(
        file_path,
        converters={
            'Input Token Attributions': parse_array_or_matrix,
            'Output Token Attributions': parse_array_or_matrix,
        }
    )

    return df

def read_all_attribution_scores(): 
    mistral = 'Mistral/attribution_results_mistral_top_10.csv'
    bloom = 'Bloom/attribution_results_bloom_top_10.csv'
    llama = 'Llama/attribution_results_llama_top_10.csv'

    mistral_df = read_attribution_scores(mistral)
    mistral_df['Model'] = 'Mistral'

    bloom_df = read_attribution_scores(bloom)
    bloom_df['Model'] = 'Bloom'

    llama_df = read_attribution_scores(llama)
    llama_df['Model'] = 'Llama'

    combined_df = pd.concat([mistral_df, bloom_df, llama_df], ignore_index=True)

    return combined_df    


df = read_all_attribution_scores()
df.head()


# load tokenizers and get tokens for all the prompts
login("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")
llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")

login("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")
mistral_tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1") 

bloom_tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-7b1")

tokenizers = {'Llama':llama_tokenizer,
              "Mistral":mistral_tokenizer, 
              "Bloom":bloom_tokenizer}


def plot_token_attributions(input_tokens, attributions, title="Token Attribution Visualization"):
    plt.figure(figsize=(12, 6))
    bar_colors = ["blue" if float(score) > 0 else "red" for score in attributions]

    plt.bar(range(len(input_tokens)), attributions, color=bar_colors, alpha=0.7)
    plt.xticks(range(len(input_tokens)), input_tokens, rotation=90)
    plt.xlabel("Input Tokens")
    plt.ylabel("Attribution Score")
    plt.title(title)
    plt.tight_layout()
    plt.show()

# Helper function to visualize attribution heatmap
def plot_attribution_heatmap(input_tokens, output_tokens, attributions, title="Attribution Heatmap"):
    plt.figure(figsize=(10, 15))
    sns.heatmap(
        attributions,
        annot=False,
        xticklabels=input_tokens,
        yticklabels=output_tokens,
        cmap="coolwarm",
        cbar_kws={"label": "Attribution Score"}
    )
    plt.xlabel("Input Tokens")
    plt.ylabel("Output Tokens")
    plt.title(title)
    plt.tight_layout()
    plt.show()

def create_clean_token(text, tokenizer): 
    tokens = tokenizer.tokenize(text)
    ret = [token.replace('Ġ', '').replace('▁', '').replace('##', '').replace("<0x0A>", "[NL]") for token in tokens if token not in tokenizer.all_special_tokens]
    assert(len(ret) == len(tokens))
    return ret
    # return [token.replace('Ġ', '').replace('▁', '').replace('##', '') for token in tokens]

# Generate visualizations for a specific row in the attribution results DataFrame
def visualize_attributions(row, tokenizers):
    input_tokens = create_clean_token(row['Prompt'], tokenizers[row['Model']])
    output_tokens = create_clean_token(row['Response'], tokenizers[row['Model']])
    print(output_tokens)
    input_attributions = row["Input Token Attributions"]
    output_attributions = row["Output Token Attributions"]

    print(f"Visualizing for Prompt: {row['Prompt']}")
    print(f"Response: {row['Response']}")

    # Token-level attribution visualization
    plot_token_attributions(input_tokens, input_attributions, title="Input Token Attributions")

    # Attribution heatmap
    plot_attribution_heatmap(
        input_tokens,
        output_tokens,
        output_attributions,
        title="Input-Output Attribution Heatmap"
    )

for idx, row in df.iterrows():
    visualize_attributions(row, tokenizers)
    if idx == 5: 
        break



# visualize the top attribution tokens 
def visualize_top_attributed_tokens_input(df, tokenizers):
    all_tokens = []
    all_scores = []
    all_models = []

    for _, row in df.iterrows():
        model = row['Model']
        tokenizer = tokenizers[model]

        prompt_tokens = create_clean_token(row['Prompt'], tokenizer)
        input_scores = np.array(row['Input Token Attributions'])

        try: 
            assert(len(prompt_tokens) == len(input_scores))
        except (AssertionError): 
            print(f"Mismatch with number of scores and response tokens for model {model}")
            print(f"{len(input_scores)} Scores, {len(prompt_tokens)} Input Tokens\n")
            if (len(prompt_tokens) < len(input_scores)): 
                input_scores = input_scores[:len(prompt_tokens)]
            else: 
                prompt_tokens = prompt_tokens[:len(input_scores)]

        top_indices = np.argsort(-np.abs(input_scores))[:10]
        for idx in top_indices:
            all_tokens.append(prompt_tokens[idx])
            all_scores.append(input_scores[idx])
            all_models.append(model)

    top_df = pd.DataFrame({'Token': all_tokens, 'Attribution Score': all_scores, 'Model': all_models})
    top_df_grouped = top_df.groupby('Token').agg({'Attribution Score': 'mean'}).reset_index()
    top_10 = top_df_grouped.nlargest(10, 'Attribution Score')

    plt.figure(figsize=(12, 6))
    plt.barh(top_10['Token'], top_10['Attribution Score'], color='skyblue')
    plt.xlabel('Attribution Score')
    plt.ylabel('Tokens')
    plt.title('Top 10 Tokens with Highest Attribution Scores')
    plt.gca().invert_yaxis()  # Invert the y-axis for better readability
    plt.show()



visualize_top_attributed_tokens_input(df, tokenizers)


def visualize_top_attributed_tokens_output(df, tokenizers):
    all_tokens = []
    all_scores = []
    all_models = []

    for _, row in df.iterrows():
        model = row['Model']
        tokenizer = tokenizers[model]

        response_tokens = create_clean_token(row['Response'], tokenizer)
        output_scores_matrix = np.array(row['Output Token Attributions'])

        try:
            assert(output_scores_matrix.shape[0] == len(response_tokens))
        except (AssertionError):
            print(f"Mismatch with number of scores and response tokens for model {model}")
            print(f"Scores Matrix Shape: {output_scores_matrix.shape}, Response Tokens: {len(response_tokens)}\n")
            if output_scores_matrix.shape[0] > len(response_tokens):
                output_scores_matrix = output_scores_matrix[:len(response_tokens), :]
            else:
                response_tokens = response_tokens[:output_scores_matrix.shape[0]]

        output_scores = np.sum(output_scores_matrix, axis=1)
        top_indices = np.argsort(-np.abs(output_scores))[:10]
        for idx in top_indices:
            all_tokens.append(response_tokens[idx])
            all_scores.append(output_scores[idx])
            all_models.append(model)

    top_df = pd.DataFrame({'Token': all_tokens, 'Attribution Score': all_scores, 'Model': all_models})
    top_df_grouped = top_df.groupby('Token').agg({'Attribution Score': 'mean'}).reset_index()
    top_10 = top_df_grouped.nlargest(10, 'Attribution Score')

    plt.figure(figsize=(12, 6))
    plt.barh(top_10['Token'], top_10['Attribution Score'], color='salmon')
    plt.xlabel('Attribution Score')
    plt.ylabel('Tokens')
    plt.title('Top 10 Response Tokens with Highest Attribution Scores')
    plt.gca().invert_yaxis()  # Invert the y-axis for better readability
    plt.show()


visualize_top_attributed_tokens_output(df, tokenizers)



def plot_low_attribution_overrepresentation(df, tokenizers, low_attribution_threshold=0.1):
    all_tokens = []
    all_scores = []
    token_frequencies = Counter()

    for _, row in df.iterrows():
        model = row['Model']
        tokenizer = tokenizers[model]

        prompt_tokens = create_clean_token(row['Prompt'], tokenizer)
        response_tokens = create_clean_token(row['Response'], tokenizer)

        input_scores = np.array(row['Input Token Attributions'])
        output_scores_matrix = np.array(row['Output Token Attributions'])

        try: 
            assert(len(prompt_tokens) == len(input_scores))
            assert(output_scores_matrix.shape[0] == len(response_tokens))
        except (AssertionError): 
            print(f"Mismatch with number of scores and response tokens for model {model}")
            print(f"{len(input_scores)} Scores, {len(prompt_tokens)} Input Tokens\n")
            print(f"Scores Matrix Shape: {output_scores_matrix.shape}, Response Tokens: {len(response_tokens)}")

            if (len(prompt_tokens) < len(input_scores)): 
                input_scores = input_scores[:len(prompt_tokens)]
            elif (len(prompt_tokens) > len(input_scores)): 
                prompt_tokens = prompt_tokens[:len(input_scores)]

            if output_scores_matrix.shape[0] > len(response_tokens):
                output_scores_matrix = output_scores_matrix[:len(response_tokens), :]
            elif output_scores_matrix.shape[0] < len(response_tokens):
                response_tokens = response_tokens[:output_scores_matrix.shape[0]]

        output_scores = np.sum(output_scores_matrix, axis=1)
        low_input_indices = np.where(np.abs(input_scores) <= low_attribution_threshold)[0]
        low_output_indices = np.where(np.abs(output_scores) <= low_attribution_threshold)[0]

        all_tokens.extend([prompt_tokens[i] for i in low_input_indices])
        all_scores.extend([input_scores[i] for i in low_input_indices])
        all_tokens.extend([response_tokens[i] for i in low_output_indices])
        all_scores.extend([output_scores[i] for i in low_output_indices])

        token_frequencies.update(prompt_tokens)
        token_frequencies.update(response_tokens)

    token_data = [
        {'Token': token, 'Attribution Score': score, 'Frequency': token_frequencies[token]}
        for token, score in zip(all_tokens, all_scores)
    ]
    low_attribution_df = pd.DataFrame(token_data)

    grouped = low_attribution_df.groupby('Token').agg(
        {'Attribution Score': 'mean', 'Frequency': 'sum'}
    ).reset_index()

    grouped = grouped.sort_values(by='Frequency', ascending=False).head(10)

    fig, ax1 = plt.subplots(figsize=(12, 6))
    ax1.bar(grouped['Token'], grouped['Frequency'], color='skyblue', alpha=0.7, label='Frequency')
    ax1.set_xlabel('Tokens')
    ax1.set_ylabel('Frequency', color='skyblue')
    ax1.tick_params(axis='y', labelcolor='skyblue')

    ax2 = ax1.twinx()
    ax2.plot(grouped['Token'], grouped['Attribution Score'], color='salmon', marker='o', label='Attribution Score')
    ax2.set_ylabel('Attribution Score', color='salmon')
    ax2.tick_params(axis='y', labelcolor='salmon')

    plt.title('Low Attribution Tokens and Their Overrepresentation')
    fig.tight_layout()
    plt.show()



plot_low_attribution_overrepresentation(df, tokenizers, low_attribution_threshold=0.3)


def plot_high_attribution_overrepresentation(df, tokenizers, high_attribution_threshold=1.0, frequency_threshold=5):
    all_tokens = []
    all_scores = []
    token_frequencies = Counter()

    # Process each row in the DataFrame
    for _, row in df.iterrows():
        model = row['Model']
        tokenizer = tokenizers[model]

        # Tokenize input and response

        prompt_tokens = create_clean_token(row['Prompt'], tokenizer)
        response_tokens = create_clean_token(row['Response'], tokenizer)

        # Extract attributions
        input_scores = np.array(row['Input Token Attributions'])
        output_scores_matrix = np.array(row['Output Token Attributions'])

        try: 
            assert(len(prompt_tokens) == len(input_scores))
            assert(output_scores_matrix.shape[0] == len(response_tokens))
        except (AssertionError): 
            print(f"Mismatch with number of scores and response tokens for model {model}")
            print(f"{len(input_scores)} Scores, {len(prompt_tokens)} Input Tokens\n")
            print(f"Scores Matrix Shape: {output_scores_matrix.shape}, Response Tokens: {len(response_tokens)}")

            if (len(prompt_tokens) < len(input_scores)): 
                input_scores = input_scores[:len(prompt_tokens)]
            elif (len(prompt_tokens) > len(input_scores)): 
                prompt_tokens = prompt_tokens[:len(input_scores)]

            if output_scores_matrix.shape[0] > len(response_tokens):
                output_scores_matrix = output_scores_matrix[:len(response_tokens), :]
            elif output_scores_matrix.shape[0] < len(response_tokens):
                response_tokens = response_tokens[:output_scores_matrix.shape[0]]

        # Sum output scores across input tokens
        output_scores = np.sum(output_scores_matrix, axis=1)

        # Identify high attribution tokens
        high_input_indices = np.where(np.abs(input_scores) >= high_attribution_threshold)[0]
        high_output_indices = np.where(np.abs(output_scores) >= high_attribution_threshold)[0]

        # Add high attribution tokens and their scores
        all_tokens.extend([prompt_tokens[i] for i in high_input_indices])
        all_scores.extend([input_scores[i] for i in high_input_indices])
        all_tokens.extend([response_tokens[i] for i in high_output_indices])
        all_scores.extend([output_scores[i] for i in high_output_indices])

        # Count token frequencies
        token_frequencies.update(prompt_tokens)
        token_frequencies.update(response_tokens)

    # Create a DataFrame for high attribution tokens
    token_data = [
        {'Token': token, 'Attribution Score': score, 'Frequency': token_frequencies[token]}
        for token, score in zip(all_tokens, all_scores)
    ]
    high_attribution_df = pd.DataFrame(token_data)

    # Filter by frequency
    high_attribution_df = high_attribution_df[high_attribution_df['Frequency'] >= frequency_threshold]

    # Group by token and compute average attribution score and total frequency
    grouped = high_attribution_df.groupby('Token').agg(
        {'Attribution Score': 'mean', 'Frequency': 'sum'}
    ).reset_index()

    # Sort by frequency
    grouped = grouped.sort_values(by='Frequency', ascending=False).head(10)

    # Plot the data
    fig, ax1 = plt.subplots(figsize=(12, 6))

    # Bar plot for frequency
    ax1.bar(grouped['Token'], grouped['Frequency'], color='skyblue', alpha=0.7, label='Frequency')
    ax1.set_xlabel('Tokens')
    ax1.set_ylabel('Frequency', color='skyblue')
    ax1.tick_params(axis='y', labelcolor='skyblue')

    # Line plot for attribution score
    ax2 = ax1.twinx()
    ax2.plot(grouped['Token'], grouped['Attribution Score'], color='salmon', marker='o', label='Attribution Score')
    ax2.set_ylabel('Attribution Score', color='salmon')
    ax2.tick_params(axis='y', labelcolor='salmon')

    # Title and legends
    plt.title('High Attribution Tokens and Their Overrepresentation')
    fig.tight_layout()
    plt.show()


plot_high_attribution_overrepresentation(df, tokenizers, high_attribution_threshold=1.0, frequency_threshold=5)


def plot_high_attribution_low_frequency(df, tokenizers, high_attribution_threshold=1.0, frequency_threshold=2):
    all_tokens = []
    all_scores = []
    token_frequencies = Counter()

    for _, row in df.iterrows():
        model = row['Model']
        tokenizer = tokenizers[model]

        # Tokenize prompt and response
        prompt_tokens = create_clean_token(row['Prompt'], tokenizer)
        response_tokens = create_clean_token(row['Response'], tokenizer)

        # Extract attribution scores
        input_scores = np.array(row['Input Token Attributions'])
        output_scores_matrix = np.array(row['Output Token Attributions'])

        try:
            assert len(prompt_tokens) == len(input_scores)
            assert output_scores_matrix.shape[0] == len(response_tokens)
        except (AssertionError):
            print(f"Mismatch with number of scores and response tokens for model {model}")
            print(f"{len(input_scores)} Scores, {len(prompt_tokens)} Input Tokens\n")
            print(f"Scores Matrix Shape: {output_scores_matrix.shape}, Response Tokens: {len(response_tokens)}")

            # Adjust lengths if mismatched
            if len(prompt_tokens) < len(input_scores):
                input_scores = input_scores[:len(prompt_tokens)]
            elif len(prompt_tokens) > len(input_scores):
                prompt_tokens = prompt_tokens[:len(input_scores)]

            if output_scores_matrix.shape[0] > len(response_tokens):
                output_scores_matrix = output_scores_matrix[:len(response_tokens), :]
            elif output_scores_matrix.shape[0] < len(response_tokens):
                response_tokens = response_tokens[:output_scores_matrix.shape[0]]

        # Sum output scores across input tokens
        output_scores = np.sum(output_scores_matrix, axis=1)

        # Identify high attribution tokens
        high_input_indices = np.where(np.abs(input_scores) >= high_attribution_threshold)[0]
        high_output_indices = np.where(np.abs(output_scores) >= high_attribution_threshold)[0]

        # Add high attribution tokens and their scores
        all_tokens.extend([prompt_tokens[i] for i in high_input_indices])
        all_scores.extend([input_scores[i] for i in high_input_indices])
        all_tokens.extend([response_tokens[i] for i in high_output_indices])
        all_scores.extend([output_scores[i] for i in high_output_indices])

        # Count token frequencies
        token_frequencies.update(prompt_tokens)
        token_frequencies.update(response_tokens)

    # Create a DataFrame for high attribution tokens
    token_data = [
        {'Token': token, 'Attribution Score': score, 'Frequency': token_frequencies[token]}
        for token, score in zip(all_tokens, all_scores)
    ]
    high_attribution_df = pd.DataFrame(token_data)

    # Filter for low-frequency tokens
    low_frequency_df = high_attribution_df[high_attribution_df['Frequency'] <= frequency_threshold]

    # Group by token and compute average attribution score and total frequency
    grouped = low_frequency_df.groupby('Token').agg(
        {'Attribution Score': 'mean', 'Frequency': 'sum'}
    ).reset_index()

    # Sort by attribution score or frequency for visualization
    grouped = grouped.sort_values(by='Attribution Score', ascending=False).head(10)

    # Plot the data
    fig, ax1 = plt.subplots(figsize=(12, 6))

    # Bar plot for frequency
    ax1.bar(grouped['Token'], grouped['Frequency'], color='skyblue', alpha=0.7, label='Frequency')
    ax1.set_xlabel('Tokens')
    ax1.set_ylabel('Frequency', color='skyblue')
    ax1.tick_params(axis='y', labelcolor='skyblue')

    # Line plot for attribution score
    ax2 = ax1.twinx()
    ax2.plot(grouped['Token'], grouped['Attribution Score'], color='salmon', marker='o', label='Attribution Score')
    ax2.set_ylabel('Attribution Score', color='salmon')
    ax2.tick_params(axis='y', labelcolor='salmon')

    # Title and legends
    plt.title('High Attribution Tokens with Low Frequency')
    fig.tight_layout()
    plt.show()


plot_high_attribution_low_frequency(df, tokenizers, high_attribution_threshold=1.0, frequency_threshold=2)



nlp = spacy.load("en_core_web_sm")

def pos_tag_prompt(prompt):
    doc = nlp(prompt)
    pos_mapping = {token: token.pos_ for token in doc}
    
    return pos_mapping

def build_token_pos_attribution_mapping(df):
    token_mapping = {}

    for _, row in df.iterrows():
        prompt_pos = pos_tag_prompt(row['Prompt'])
        response_pos = pos_tag_prompt(row['Response'])

        # Extract attribution scores for input and output
        input_scores = np.array(row['Input Token Attributions'])
        output_scores_matrix = np.array(row['Output Token Attributions'])

        # Tokenize the prompt and response
        prompt_tokens = prompt_pos.keys() 
        response_tokens = response_pos.keys() 

        # Ensure matching lengths
        # if len(prompt_tokens) > len(input_scores):
        #     prompt_tokens = prompt_tokens[:len(input_scores)]
        # if len(response_tokens) > output_scores_matrix.shape[0]:
        #     response_tokens = response_tokens[:output_scores_matrix.shape[0]]

        # Combine prompt and response tokens
        all_tokens = list(prompt_tokens) + list(response_tokens)
        all_pos = {**prompt_pos, **response_pos}
        all_scores = list(input_scores) + list(np.sum(output_scores_matrix, axis=1))

        # Populate the token_mapping dictionary
        for token, pos, score in zip(all_tokens, all_pos.values(), all_scores):
            if token.text not in token_mapping:
                token_mapping[token.text] = {}

            if pos not in token_mapping[token.text]:
                token_mapping[token.text][pos] = []

            token_mapping[token.text][pos].append(score)

    return token_mapping


token_pos_attribution_mapping = build_token_pos_attribution_mapping(df)
# Loop through the token_pos_attribution_mapping to print the number of POS taggings per token
tag = []
for token, pos_mapping in token_pos_attribution_mapping.items():
    num_pos_tags = len(pos_mapping)  # Count how many unique POS tags are associated with the token
    tag.append((token, num_pos_tags))

print(len(tag))
tag = [t for t in tag if t[1] > 2] 
print(len(tag))
print(sorted(tag, reverse=True, key=lambda x: x[1]))



# def correlation_pos_attribution(token_pos_attribution_mapping):
#     filtered_tokens = [
#         (token, pos_mapping)
#         for token, pos_mapping in token_pos_attribution_mapping.items()
#         if len(pos_mapping) > 2
#     ]
    
#     pos_correlation = {}

#     # For each token, calculate the correlation between POS tags and attribution scores
#     for token, pos_mapping in filtered_tokens:
#         # Collect all attribution scores and corresponding POS tags
#         pos_tags = []
#         attribution_scores = []

#         for pos, scores in pos_mapping.items():
#             # Add the scores and corresponding POS tags to lists
#             pos_tags.extend([pos] * len(scores))  # Repeat the POS tag for each score
#             attribution_scores.extend(scores)  # Append the scores

#         # Calculate the correlation between POS tags and attribution scores
#         # Convert POS tags to numerical values for correlation calculation
#         pos_numeric = [hash(pos) for pos in pos_tags]  # Simple numeric conversion for POS
#         print(pos_numeric, pos_tags)

#         # Compute the correlation using Spearman's rank correlation
#         correlation, _ = spearmanr(pos_numeric, attribution_scores)

#         # Store the result
#         pos_correlation[token] = correlation

#     # Return the dictionary containing correlations for each token
#     return pos_correlation

# # Example usage
# # Assuming `token_pos_attribution_mapping` is your dictionary with tokens, POS tags, and attribution scores
# correlations = correlation_pos_attribution(token_pos_attribution_mapping)

# # Print the correlations
# print(correlations)



def plot_token_pos_attribution(token_pos_attribution_mapping):
    filtered_tokens = [
        (token, pos_mapping)
        for token, pos_mapping in token_pos_attribution_mapping.items()
        if len(pos_mapping) > 1
    ]
    avg_tokens = [
        (token, [np.average(mapping) for _, mapping in pos_mapping]) for token, pos_mapping in token_pos_attribution_mapping
    ]

    score_arr = []
    for token, averages in avg_tokens:
        averages = np.array(averages)
        averages *= np.sum(averages)
        score = 0 
        for avg in averages:
            score +=  (avg - (1/len(averages))) ** 2
        score_arr.append((token, score))

    for token, pos_mapping in filtered_tokens:
        # Get POS tags and their corresponding average attribution scores
        pos_tags = list(pos_mapping.keys())
        avg_scores = [np.mean(scores) for scores in pos_mapping.values()]

        # Plot the data
        plt.figure(figsize=(8, 6))
        plt.bar(pos_tags, avg_scores, color='skyblue', alpha=0.7)
        
        # Add labels and title
        plt.xlabel('Part of Speech (POS)')
        plt.ylabel('Average Attribution Score')
        plt.title(f"Token: {token} - POS Attribution Averages")
        
        # Add values on top of the bars
        for i, score in enumerate(avg_scores):
            plt.text(i, score, f"{score:.2f}", ha='center', va='bottom')

        # Show the plot
        plt.tight_layout()
        plt.show()
plot_token_pos_attribution(token_pos_attribution_mapping)


def plot_ttest_heatmaps(token_pos_attribution_mapping, p_value_threshold=0.05):
    """
    For each token, performs pairwise t-tests between POS tags and creates a heatmap
    showing whether the average attribution scores for the POS tags are significantly different.

    Parameters:
        token_pos_attribution_mapping (dict): A dictionary where each token maps to a dictionary of POS tags 
                                              and attribution scores.
        p_value_threshold (float): The p-value threshold for significance (default is 0.05).
    """
    # Filter tokens with more than 2 POS tags
    filtered_tokens = [
        (token, pos_mapping)
        for token, pos_mapping in token_pos_attribution_mapping.items()
        if len(pos_mapping) > 2
    ]

    for token, pos_mapping in filtered_tokens:
        pos_tags = list(pos_mapping.keys())
        scores = list(pos_mapping.values())
        
        # Initialize a matrix to store p-values
        n = len(pos_tags)
        p_value_matrix = np.ones((n, n))  # Default p-values to 1 (no difference)

        # Perform pairwise t-tests
        for i in range(n):
            for j in range(i + 1, n):
                # Extract the scores for the two POS tags
                pos1_scores = scores[i]
                pos2_scores = scores[j]

                # Perform t-test
                try:
                    stat, p_value = ttest_ind(pos1_scores, pos2_scores, equal_var=False)
                except ValueError:
                    p_value = 1.0  # Default to no significant difference if test fails

                # Store p-value in the matrix
                p_value_matrix[i, j] = p_value
                p_value_matrix[j, i] = p_value

        # Create a heatmap for this token
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            p_value_matrix < p_value_threshold,  # Boolean matrix (True = significant)
            annot=True,  # Show the True/False values
            xticklabels=pos_tags,
            yticklabels=pos_tags,
            cmap='coolwarm',
            cbar=False
        )
        plt.title(f"T-Test Heatmap for Token: {token}")
        plt.xlabel("POS Tag")
        plt.ylabel("POS Tag")
        plt.show()
plot_ttest_heatmaps(token_pos_attribution_mapping, p_value_threshold=0.05)
